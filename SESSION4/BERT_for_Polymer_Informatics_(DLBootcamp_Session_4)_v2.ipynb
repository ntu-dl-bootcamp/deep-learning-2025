{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ntu-dl-bootcamp/deep-learning-2025/blob/main/SESSION4/BERT_for_Polymer_Informatics_DLBootcamp_Session_4_v2.ipynb)"
      ],
      "metadata": {
        "id": "iVfE9L7Xbces"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding SMILES and pSMILES\n",
        "\n",
        "### 1Ô∏è‚É£ What is SMILES?\n",
        "SMILES (Simplified Molecular Input Line Entry System) is a widely used notation for representing molecular structures as text. It encodes atoms, bonds, and connectivity in a linear format.\n",
        "\n",
        "**Example SMILES Representations:**\n",
        "- Ethanol: `CCO`\n",
        "- Benzene: `c1ccccc1`\n",
        "- Aspirin: `CC(=O)Oc1ccccc1C(=O)O`\n",
        "\n",
        "### 2Ô∏è‚É£ What is pSMILES?\n",
        "pSMILES (Polymer SMILES) extends the standard SMILES notation to describe polymers. Since polymers are repeating units, pSMILES represents them using *dummy atoms* (`[*]`), indicating where polymerization occurs.\n",
        "\n",
        "**Example pSMILES Representations:**\n",
        "- Polyethylene: `[*]CC[*]`\n",
        "- Polystyrene: `[*]CC(c1ccccc1)[*]`\n",
        "- Nylon-6: `[*]NCCCCCC(=O)[*]`\n",
        "\n",
        "Unlike small molecules, polymers contain repeating units that make traditional descriptors ineffective. This is why we use **polyBERT**, a Transformer-based model trained on pSMILES, to learn polymer representations.\n",
        "\n",
        "In this section, we will **visualize these SMILES and pSMILES** structures using RDKit.\n"
      ],
      "metadata": {
        "id": "ahk06oKmvB7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries if not already installed\n",
        "!pip install rdkit-pypi --quiet\n",
        "!pip install transformers torch --quiet"
      ],
      "metadata": {
        "id": "uwUwRmfmvDOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import Draw\n",
        "\n",
        "# Define small molecules and polymers\n",
        "smiles_examples = {\n",
        "    \"Ethanol\": \"CCO\",\n",
        "    \"Benzene\": \"c1ccccc1\",\n",
        "    \"Aspirin\": \"CC(=O)Oc1ccccc1C(=O)O\"\n",
        "}\n",
        "\n",
        "psmiles_examples = {\n",
        "    \"Polyethylene\": \"[*]CC[*]\",\n",
        "    \"Polystyrene\": \"[*]CC(c1ccccc1)[*]\",\n",
        "    \"Nylon-6\": \"[*]NCCCCCC(=O)[*]\"\n",
        "}"
      ],
      "metadata": {
        "id": "2UgGQz7ZvFVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing Small Molecules**  \n",
        "Here, we generate and display molecular structures for standard SMILES representations using RDKit.\n"
      ],
      "metadata": {
        "id": "DNYBcpSbwb8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert SMILES to RDKit molecules\n",
        "smiles_mols = [Chem.MolFromSmiles(smi) for smi in smiles_examples.values()]\n",
        "print(\"Small Molecules:\")\n",
        "Draw.MolsToGridImage(smiles_mols, molsPerRow=3, legends=list(smiles_examples.keys()))"
      ],
      "metadata": {
        "id": "OczuXnA5vp7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizing Polymers (pSMILES)**  \n",
        "Now, we visualize polymer repeat units using pSMILES. The dummy atoms (`[*]`) indicate where polymerization occurs.\n"
      ],
      "metadata": {
        "id": "BJcPvhbPwcjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert SMILES to RDKit molecules\n",
        "psmiles_mols = [Chem.MolFromSmiles(smi.replace(\"[*]\", \"*\")) for smi in psmiles_examples.values()]\n",
        "print(\"\\nPolymers (pSMILES):\")\n",
        "Draw.MolsToGridImage(psmiles_mols, molsPerRow=3, legends=list(psmiles_examples.keys()))"
      ],
      "metadata": {
        "id": "GZmDRGfVvsaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing pSMILES with polyBERT\n",
        "\n",
        "### 1Ô∏è‚É£ Why Do We Need Tokenization?  \n",
        "In Natural Language Processing (NLP), tokenization breaks text into smaller units (tokens) for model training. Similarly, for polymer informatics, we need to convert **pSMILES strings** into tokens before processing them with polyBERT.\n",
        "\n",
        "### 2Ô∏è‚É£ How Does polyBERT Tokenize pSMILES?  \n",
        "polyBERT follows a two-step process for tokenizing polymer SMILES (pSMILES):\n",
        "\n",
        "#### **Step 1: Canonicalization**  \n",
        "- pSMILES strings are first converted into a **canonical form** to maintain consistency.\n",
        "- Example transformation: `[*]CCOCCO[*]` to `[*]COC[*]`\n",
        "- This ensures that structurally identical polymers always have the same representation.\n",
        "\n",
        "#### **Step 2: Tokenization Using SentencePiece**  \n",
        "- polyBERT uses **SentencePiece**, a subword tokenizer, to split pSMILES into **265 unique tokens**.\n",
        "- These tokens include:\n",
        "- **Common chemical symbols** (e.g., elements like `C`, `O`, `N`, `H`)\n",
        "- **Numerical digits** (`0-9`)\n",
        "- **Special characters** (`[*]`, `(`, `)`, `=`, `.`)\n",
        "- This approach ensures **full coverage of the polymer SMILES vocabulary**.\n",
        "\n",
        "Now, we will **tokenize a sample pSMILES string** and examine how the tokenizer splits it into tokens.\n"
      ],
      "metadata": {
        "id": "cRHe37CPy3kK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load polyBERT tokenizer from Hugging Face\n",
        "tokenizer = AutoTokenizer.from_pretrained('kuelumbus/polyBERT')\n",
        "\n",
        "# Define sample pSMILES strings\n",
        "psmiles_examples = [\n",
        "    \"[*]CC[*]\",         # Polyethylene\n",
        "    \"[*]NCCCCCC(=O)[*]\",        # Nylon-6\n",
        "    \"[*]CC(c1ccccc1)[*]\" # Polystyrene\n",
        "]\n",
        "\n",
        "# Tokenize the pSMILES sequences\n",
        "encoded_input = tokenizer(psmiles_examples, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Display tokenized output\n",
        "print(\"-\" * 50)\n",
        "for i, psmiles in enumerate(psmiles_examples):\n",
        "    print(f\"Original pSMILES: {psmiles}\")\n",
        "    print(f\"Tokenized: {tokenizer.tokenize(psmiles)}\")\n",
        "    print(f\"Token IDs: {encoded_input['input_ids'][i].tolist()}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "P6ORFBSBzi4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exploring Tokenization in polyBERT\n",
        "\n",
        "polyBERT uses **SentencePiece** to tokenize pSMILES, breaking them into subword units.  \n",
        "\n",
        "üîπ **`_` (underscore)** appears at the start of the sequence as a [word boundary marker](https://github.com/huggingface/transformers/issues/5087).  \n",
        "üîπ The tokenizer handles polymer-specific symbols like `[*]`, ensuring consistent representation.  \n",
        "\n",
        "\\\\\n",
        "\n",
        "üîç **Explore Further**: Check the polyBERT tokenizer script:  \n",
        "üîó [polyBERT Tokenizer Code](https://github.com/Ramprasad-Group/polyBERT/blob/main/polyBERT/train_tokenizer.py)  "
      ],
      "metadata": {
        "id": "WYqALy9y2IQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating Polymer Fingerprints with polyBERT\n",
        "\n",
        "Now that we have tokenized **pSMILES**, we need to generate **numerical representations (fingerprints)** for these sequences.\n",
        "\n",
        "### üîπ How Are Fingerprints Generated?  \n",
        "- Each token from the **polyBERT tokenizer** is mapped to an **embedding**.\n",
        "- Since polyBERT **does not use the `[CLS]` token**, we compute the final sequence representation using **mean pooling**.\n",
        "- **Mean pooling** averages the token embeddings while considering the attention mask.\n",
        "\n",
        "We will now generate and explore these **polymer fingerprints**.\n"
      ],
      "metadata": {
        "id": "U2ZIHcAL3UfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Function for Mean Pooling\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "# Load polyBERT tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained('kuelumbus/polyBERT')\n",
        "model = AutoModel.from_pretrained('kuelumbus/polyBERT')\n",
        "\n",
        "# Define sample pSMILES strings\n",
        "psmiles_examples = [\n",
        "    \"[*]CC[*]\",         # Polyethylene\n",
        "    \"[*]NCCCCCC(=O)[*]\",        # Nylon-6\n",
        "    \"[*]CC(c1ccccc1)[*]\" # Polystyrene\n",
        "]\n",
        "\n",
        "# Tokenize input\n",
        "encoded_input = tokenizer(psmiles_examples, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "# Generate token embeddings\n",
        "with torch.no_grad():\n",
        "    model_output = model(**encoded_input)\n",
        "\n",
        "# Compute fingerprints using mean pooling\n",
        "fingerprints = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "\n",
        "# Display results\n",
        "print(\"Fingerprint vectors:\")\n",
        "print(fingerprints)\n"
      ],
      "metadata": {
        "id": "6UvW7awf5tIq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the Fingerprints\n",
        "\n",
        "Now that we have generated **polymer fingerprints**, explore the following:\n",
        "\n",
        "üîπ **What is the shape of the fingerprints?**  Check `fingerprints.shape`.\n",
        "\n",
        "üîπ **Does the tokenizer contain the `[CLS]` token?**  Try printing `tokenizer.cls_token`. What do you observe?\n",
        "\n",
        "üîπ **How are embeddings generated without `[CLS]`?**  Since polyBERT does not rely on `[CLS]`, why does it use **mean pooling** instead?\n",
        "\n",
        "üí° **Hint:** Mean pooling ensures that the final fingerprint represents the entire polymer sequence."
      ],
      "metadata": {
        "id": "G17fUj2q6syM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Fingerprint shape:\", fingerprints.shape)"
      ],
      "metadata": {
        "id": "7f74lfil5791"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the Relationship between Polymer Fingerprints and their Properties\n",
        "\n",
        "Now that we have our dataset, we will generate **polymer fingerprints** using polyBERT.  \n",
        "\n",
        "### Steps Involved\n",
        "1Ô∏è‚É£ Load the dataset containing polymer SMILES and their **bandgap chain values (Egc)**.  \n",
        "2Ô∏è‚É£ Tokenize each polymer SMILES and extract a fingerprint representation using polyBERT with mean pooling.  \n",
        "3Ô∏è‚É£ Visualize the embeddings to see if they reflect polymer properties, specifically their bandgap (Egc) values.  \n",
        "\n",
        "These embeddings capture structural and chemical information about each polymer, helping us understand the **relationship between polymer structure and electronic properties**.  "
      ],
      "metadata": {
        "id": "KlhrnyEKBKp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.manifold import TSNE\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Check if GPU is available and set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained('kuelumbus/polyBERT')\n",
        "model = AutoModel.from_pretrained('kuelumbus/polyBERT').to(device)\n",
        "\n",
        "# Load dataset\n",
        "url = \"https://github.com/ChangwenXu98/TransPolymer/raw/master/data/Egc.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Extract pSMILES and property values\n",
        "psmiles_list = df[\"smiles\"].tolist()\n",
        "property_values = df[\"value\"].tolist()"
      ],
      "metadata": {
        "id": "q_EaFlua8GjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define batch size\n",
        "batch_size = 64  # Adjust this value based on your GPU memory capacity\n",
        "fingerprints_list = []\n",
        "\n",
        "# Process in batches\n",
        "for i in tqdm(range(0, len(psmiles_list), batch_size), desc=\"Processing Batches\"):\n",
        "    batch_psmiles = psmiles_list[i:i+batch_size]  # Get batch\n",
        "    encoded_input = tokenizer(batch_psmiles, padding=True, truncation=True, return_tensors=\"pt\").to(device)  # Tokenize\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)  # Get model output\n",
        "\n",
        "    # Compute fingerprints using mean pooling\n",
        "    fingerprints = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "\n",
        "    # Move to CPU and store\n",
        "    fingerprints_list.append(fingerprints.cpu().numpy())"
      ],
      "metadata": {
        "id": "pvgkpPMW8P8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate all batches into a single array\n",
        "fingerprints_np = np.vstack(fingerprints_list)\n",
        "\n",
        "# Perform t-SNE dimensionality reduction\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "embeddings_2d = tsne.fit_transform(fingerprints_np)"
      ],
      "metadata": {
        "id": "EYxZwYxC8Yqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot with color mapping based on property value\n",
        "plt.figure(figsize=(8, 6))\n",
        "sc = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=property_values, cmap=\"seismic\", alpha=0.8, s=6)\n",
        "plt.colorbar(sc, label=\"Polymer Bandgap (Chain)\")\n",
        "plt.xlabel(\"t-SNE Component 1\")\n",
        "plt.ylabel(\"t-SNE Component 2\")\n",
        "plt.title(\"t-SNE Visualization of polyBERT Embeddings\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YTe-HkQc-FHB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
