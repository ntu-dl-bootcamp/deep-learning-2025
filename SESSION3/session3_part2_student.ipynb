{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fShRSlQfDMoC"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ntu-dl-bootcamp/deep-learning-2025/blob/main/session3/session3_diffusion_model.ipynb\" target=\"_blank\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: Loading a Pretrained Model\n",
        "Now we will load a pretrained model (Resnet 18) trained on Imagenet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import the libaries we will need\n",
        "import torch\n",
        "import torchvision\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# If we have a GPU, then use it\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we will grab an image of a dog and visualize it to know what we're working with.  Here we are using urllib to download the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import urllib\n",
        "urllib.request.urlretrieve(\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
        "\n",
        "img = Image.open(\"dog.jpg\")\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4a: Inferencing a Pretrained Model\n",
        "Can you complete the code to classify the image of the dog using Resnet?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocess = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize(256),\n",
        "    torchvision.transforms.CenterCrop(224),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "x = preprocess(img)\n",
        "\n",
        "y = # TODO: calculate y by inferencing the model\n",
        "probabilities = torch.nn.functional.softmax(y[0], dim=0)\n",
        "print(probabilities)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's interpret those results.  Each dimension in the output array corresponds to one class, but Imagenet has 1000 classes.  Luckily there is a list that maps Imagenet dimension number to classes.  Let's use it to find out what we actually predicted..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download ImageNet labels\n",
        "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
        "\n",
        "# Load the file as a list in python\n",
        "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
        "    categories = [s.strip() for s in f.readlines()]\n",
        "\n",
        "# Show the top 5 most likely 5 categories\n",
        "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
        "for i in range(top5_prob.size(0)):\n",
        "    print(categories[top5_catid[i]], top5_prob[i].item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we're going to grab a dataset of different Pokemon.  This dataset comes in a zip format, so we'll use some new libraries to extract it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import io\n",
        "import requests\n",
        "\n",
        "response = requests.get('https://osf.io/u4njm/download', stream=True)\n",
        "archive = zipfile.ZipFile(io.BytesIO(response.content))\n",
        "archive.extractall()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's take a look at an image from the smaller Pok√©mon dataset. It's different from the 1000 classes that ResNet is trained to recognize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img = Image.open('small_pokemon_dataset/Charizard/d906b6b9b24a4baf9c069a27b87e15c0.jpg')\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4b: Fine-tuning Dataset\n",
        "Before we can perform transfer learning on the Pokemon dataset, we need to split between training, validation, and test.  Can you complete the code below to make that happen?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preprocess = torchvision.transforms.Compose([\n",
        "  torchvision.transforms.Resize((224, 224)),\n",
        "  torchvision.transforms.ToTensor()\n",
        "])\n",
        "\n",
        "pokemon = torchvision.datasets.ImageFolder(\n",
        "  'small_pokemon_dataset',\n",
        "  transform=preprocess\n",
        ")\n",
        "\n",
        "# TODO: Complete the function below to fill in the dataset\n",
        "train_set, val_set, test_set = torch.utils.data.random_split()\n",
        "print(f'No. Train Images: {len(train_set)}')\n",
        "print(f'No. Val Images: {len(val_set)}')\n",
        "print(f'No. Test Images: {len(test_set)}')\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_set,\n",
        "    batch_size=16,\n",
        "    shuffle=True\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_set,\n",
        "    batch_size=16\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_set,\n",
        "    batch_size=16\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Excercise 4c: Modifying our pretrained model\n",
        "Resent comes with 1000 output neurons, but we only have 9 classes.  Can you replace the last layer of the model to only have 9 outputs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "no_features = model.fc.in_features\n",
        "model.fc = # TODO: Add a linear layer with the correct number of input features and output features\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Excercise 4d: Training Loop\n",
        "Can you complete the code below to use cross-entropy loss during training?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up our optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "loss_function = # TODO: set our loss function to cross-entropy loss\n",
        "\n",
        "# Allow model gradients to be updated\n",
        "model.train()\n",
        "\n",
        "\n",
        "train_loss = []\n",
        "val_loss = []\n",
        "for epoch in range(10):\n",
        "  train_loss.append(0)\n",
        "  val_loss.append(0)\n",
        "\n",
        "  # Train loop\n",
        "  for batch in train_loader:\n",
        "    x, y = batch\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    y_hat = model(x)\n",
        "    L = loss_function(y_hat, y)\n",
        "    L.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_loss[-1] += L\n",
        "\n",
        "  # Validation loop\n",
        "  for data in val_loader:\n",
        "    x, y = data\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    with torch.no_grad():\n",
        "      y_hat = model(x)\n",
        "      L = loss_function(y_hat, y)\n",
        "      val_loss[-1] += L\n",
        "\n",
        "  print(f'Epoch: {epoch} --- Training Loss: {train_loss[-1]:.2f} --- Val Loss: {val_loss[-1]:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4e: Prediction\n",
        "Let's take an image from our dataset.  How did we do at prediction.  Can you complete the code below to find out?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img = Image.open('small_pokemon_dataset/Blastoise/03bc036b642d4873985399cebfd0bb64.jpg')\n",
        "x = preprocess(img)\n",
        "\n",
        "y = # TODO: Inference the model to make a prediction\n",
        "probabilities = torch.nn.functional.softmax(y[0], dim=0)\n",
        "for name, idx in pokemon.class_to_idx.items():\n",
        "  print(f'{name}: {100 * probabilities[idx]:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 5: Segmentation\n",
        "\n",
        "\n",
        "<table align=\"center\">\n",
        "    <tr>\n",
        "        <td align=\"center\">\n",
        "            <img src=\"https://raw.githubusercontent.com/mateuszbuda/brain-segmentation-pytorch/refs/heads/master/assets/TCGA_DU_6404_19850629.gif\" \n",
        "                 alt=\"TCGA_DU_6404_19850629\" width=\"250\">\n",
        "        </td>\n",
        "        <td align=\"center\">\n",
        "            <img src=\"https://raw.githubusercontent.com/mateuszbuda/brain-segmentation-pytorch/refs/heads/master/assets/TCGA_HT_7879_19981009.gif\" \n",
        "                 alt=\"TCGA_HT_7879_19981009\" width=\"250\">\n",
        "        </td>\n",
        "        <td align=\"center\">\n",
        "            <img src=\"https://raw.githubusercontent.com/mateuszbuda/brain-segmentation-pytorch/refs/heads/master/assets/TCGA_CS_4944_20010208.gif\" \n",
        "                 alt=\"TCGA_CS_4944_20010208\" width=\"250\">\n",
        "        </td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "        <td align=\"center\"><b>94% DSC</b></td>\n",
        "        <td align=\"center\"><b>91% DSC</b></td>\n",
        "        <td align=\"center\"><b>89% DSC</b></td>\n",
        "    </tr>\n",
        "</table>\n",
        "\n",
        "Now we will use U-Net, an early model for image segmentation to look at identifying brain tumors in CT scan images.  First we will load the model from torch hub and download an image to test on.\n",
        "\n",
        "<figure>\n",
        "<p style=\"text-align:center;\"  align = \"center\"><img src=\"https://raw.githubusercontent.com/mateuszbuda/brain-segmentation-pytorch/refs/heads/master/assets/unet.png\" alt=\"Trulli\" style=\"width:100%\"  align = \"center\"></p>\n",
        "<figcaption align = \"center\">U-Net Architecture for Brain Segmentation<a href=\"https://github.com/mateuszbuda/brain-segmentation-pytorch\"> Official GitHub Release</a> </figcaption>\n",
        "</figure>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install medpy\n",
        "import urllib.request\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from skimage import color\n",
        "from skimage.io import imsave\n",
        "from medpy.filter.binary import largest_connected_component\n",
        "from scipy.ndimage import binary_dilation\n",
        "\n",
        "# Load Model\n",
        "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
        "    in_channels=3, out_channels=1, init_features=32, pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "# Load Test Image\n",
        "urllib.request.urlretrieve(\"https://github.com/mateuszbuda/brain-segmentation-pytorch/raw/master/assets/TCGA_CS_4944.png\", \"TCGA_CS_4944.png\")\n",
        "img = Image.open(\"TCGA_CS_4944.png\").convert(\"RGB\") \n",
        "plt.imshow(img)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code to preprocess the image is provided for you below, can you preprocess the image and inference the model?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.1, 0.1, 0.1], std=[0.5, 0.5, 0.5]), \n",
        "])\n",
        "\n",
        "\n",
        "# TODO: Can you complete the code below to preprocess the image and inference the model?\n",
        "x =\n",
        "with torch.no_grad():\n",
        "    y ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can post-process the prediction to visualize if the model has successfully identified the boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "y_pred_np = y.squeeze().detach().cpu().numpy()\n",
        "y_pred_np = (y_pred_np > 0.5).astype(np.uint8)  # Thresholding\n",
        "y_pred_np = largest_connected_component(y_pred_np)  # Keep largest connected component\n",
        "\n",
        "# Convert image to numpy array\n",
        "img_np = np.array(img)\n",
        "\n",
        "# Generate Edge Map using Binary Dilation\n",
        "def create_boundary(mask):\n",
        "    edge = binary_dilation(mask) ^ mask  # Create boundary from binary mask\n",
        "    return edge\n",
        "\n",
        "# Generate boundary map\n",
        "boundary_mask = create_boundary(y_pred_np)\n",
        "\n",
        "# Overlay Boundaries on the Image\n",
        "img_with_boundary = img_np.copy()\n",
        "img_with_boundary[boundary_mask == 1] = [255, 0, 0]  # Red outline\n",
        "\n",
        "# Display results\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title(\"Segmentation Overlay (with Boundaries)\")\n",
        "plt.imshow(img_with_boundary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGUQcdqYOZ_t"
      },
      "source": [
        "# Diffusion Model\n",
        "\n",
        "[Original Notebook Source (in TensorFlow)](https://www.infocusp.com/blogs/understanding-diffusion-models-by-coding/)\n",
        "\n",
        "[Diffusion models](https://arxiv.org/pdf/2006.11239.pdf) are a family of models that have shown amazing capability of generating photorealistic images with/ without text prompt. They have two flows as shown in the figure below -\n",
        "1. Deterministic forward flow (from image to noise) and\n",
        "2. Generative reverse flow (recreating image from noise).\n",
        "\n",
        "Diffusion models get their name from the forward flow where they follow a markov chain of diffusion steps, each of which adds a small amount of random noise to the data. Then they learn the model to reverse the diffusion process and construct desired data samples from noise.\n",
        "\n",
        "<figure>\n",
        "<p style=\"text-align:center;\"  align = \"center\"><img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2022/04/Fixed_Forward_Diffusion_Process.png\" alt=\"Trulli\" style=\"width:100%\"  align = \"center\"></p>\n",
        "<figcaption align = \"center\">Forward and reverse process <a href=\"https://developer.nvidia.com/blog/improving-diffusion-models-as-an-alternative-to-gans-part-1/\">Ref: Nvidia blog</a> </figcaption>\n",
        "</figure>\n",
        "\n",
        "\n",
        "\n",
        "Since they map noise to data, these models can be said to be capable of learning the distributions that generate data of any particular domain.\n",
        "\n",
        "This notebook showcases a minimal example of the forward diffusion process and its reverse mapping using a dense network. It is meant to give the reader side by side code snippets to match the equations in the paper and visual examples of the complete process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "choHKvGkOUbd"
      },
      "source": [
        "### Imports and utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0EVLOt8MwG1",
        "outputId": "e5a613b0-0d38-4d27-cf54-e49d1fc4dbbd"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "! pip install celluloid\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributions as dist\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import functools\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from celluloid import Camera\n",
        "import functools\n",
        "import sklearn.datasets\n",
        "# For plotting\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cMVVPNCoiMC"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Utility function for displaying video inline in colab\n",
        "\n",
        "def show_video(vname):\n",
        "  mp4 = open(vname,'rb').read()\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "  return HTML(\"\"\"\n",
        "  <video width=400 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "  </video>\n",
        "  \"\"\" % data_url)\n",
        "\n",
        "def save_animation(vname, interval=30):\n",
        "  anim = camera.animate(blit=True, interval=interval)\n",
        "  anim.save(vname)\n",
        "\n",
        "# Utility function for random noise\n",
        "def noise_like(shape):\n",
        "  return torch.randn(shape, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vm7lVgUvNiPB"
      },
      "source": [
        "## Data Distribution\n",
        "\n",
        "Images can be conceptualized as points sampled from a high-dimensional space with dimensions corresponding to the image's height and width.\n",
        "\n",
        "Consider an image of dimensions $height \\times width$. In such an image, the total number of pixels equals $height \\times width$, each pixel having a value ranging from 0 to 255 to represent intensity.\n",
        "\n",
        "Now, envision a vector space where we flatten this image, representing the intensity of each pixel along a single dimension of the vector. For instance, a\n",
        "$2 \\times 3$ image (2 pixels in height, 3 pixels in width) transforms into a single vector of length 6, with each component of this vector ranging from 0 to 255.\n",
        "\n",
        "Within this image vector space, we observe small clusters of valid(photorealistic) images, sparsely distributed throughout the space. Conversely, the remaining vector space comprises invalid (non-photorealistic) images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDlU_wb8T-T5"
      },
      "source": [
        "For the example in this notebook, we consider a **hypothetical** simplified version of the above representation. We consider images made of just 2 pixels, each of which can have integer values between [-5, 5]. This is to allow visualization of each dimension of the data as it moves through the forward and reverse process (and additionally faster training). Example [0, 1] is an hypothetical image for us.\n",
        "\n",
        "The same code can be extended to the original image dimensions with just updated data dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbdNUSgTNOjX",
        "outputId": "65b4d41b-698e-4511-b712-d18cc6f750d8"
      },
      "outputs": [],
      "source": [
        "# Generate original points which are around [0.5, 0.5] in all quadrants and\n",
        "# 4 corners ([0,1], [1,0], [0,-1], [-1, 0])\n",
        "# Some region around these points indicates valid images region (true data distribution)\n",
        "\n",
        "# Define parameters\n",
        "num_samples_per_center = 1000\n",
        "stddev = 0.1\n",
        "mean = 0\n",
        "\n",
        "# Create centers tensor\n",
        "centers = torch.tensor([[0, 1], [1, 0], [0, -1], [-1, 0],\n",
        "                       [0.5, 0.5], [0.5, -0.5], [-0.5, -0.5],\n",
        "                       [-0.5, 0.5]]) * 4\n",
        "print('Hypothetical Image (2 pixels) Data:\\n', centers)\n",
        "# Generate data for each cluster\n",
        "all_data = []\n",
        "for idx in range(centers.shape[0]):\n",
        "    center = centers[idx]\n",
        "    normal_dist = dist.Normal(loc=center, scale=stddev)\n",
        "    data = normal_dist.sample(torch.Size([num_samples_per_center]))\n",
        "    all_data.append(data)\n",
        "\n",
        "# Concatenate data into a single tensor\n",
        "train_data= torch.cat(all_data, dim=0)\n",
        "\n",
        "# Print information\n",
        "print(f'{train_data.shape[0]} samples of {train_data.shape[1]} dimensions in training data')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwjTwgC6Wq1h"
      },
      "source": [
        "The **x's**  in the plot below can be thought of as valid images in 2d space with the rest of the white region representing the rest of the invalid images. The blue clusters around the x's are also valid images (corresponding to minor pixel perturbations in original images)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "LJrXAdsLO6t8",
        "outputId": "3c41ae06-034a-422f-8cd1-766c07f9c3b9"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "# Visualize the data\n",
        "plt.figure(figsize=[5,5])\n",
        "plt.scatter(train_data[:,0], train_data[:,1])\n",
        "for center in centers.numpy():\n",
        "  # print(center[0], center[1])\n",
        "  plt.scatter([center[0]], [center[1]], marker='x', color='r')\n",
        "plt.title('Original Data Distribution')\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-RDRbzZsD7H"
      },
      "source": [
        "## Beta Schedule (how much noise to add at each time step of diffusion)\n",
        "\n",
        "With the original (non-noisy) data in hand, our next step is to initiate the diffusion model implementation. The initial phase involves introducing noise to the input images according to a predetermined variance schedule, often referred to as a beta schedule. The original paper adopts a linear schedule for this purpose. Additionally, the model typically progresses through 1000 timesteps, advancing both forward and backward. However, given the simplicity of our dataset, we opt for a smaller number of timesteps, specifically 250, to effectively capture the dynamics of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qimwKYzhNM6w"
      },
      "source": [
        "**Beta derivatives**\n",
        "\n",
        "Next, let's compute all the derivatives from beta that are used repeatedly in the forward and reverse process of diffusion. Since the variance schedule ($\\beta_t$) is fixed, the derivatives of $\\beta_t$ are also fixed. We precompute these to save time/ compute.\n",
        "\n",
        "We'll see the use cases of these variables in the respective sections below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa-cIhVdWkii"
      },
      "outputs": [],
      "source": [
        "num_diffusion_timesteps=250\n",
        "beta_start=0.0001\n",
        "beta_end=0.02\n",
        "schedule_type='linear'\n",
        "\n",
        "def get_beta_schedule(schedule_type, beta_start, beta_end, num_diffusion_timesteps):\n",
        "  if schedule_type == 'quadratic':\n",
        "    betas = np.linspace(beta_start ** 0.5, beta_end ** 0.5, num_diffusion_timesteps, dtype=np.float32) ** 2\n",
        "  elif schedule_type == 'linear':\n",
        "    betas = np.linspace(beta_start, beta_end, num_diffusion_timesteps, dtype=np.float32)\n",
        "  return betas\n",
        "\n",
        "betas_linear = get_beta_schedule('linear', beta_start, beta_end, num_diffusion_timesteps)\n",
        "betas_quad = get_beta_schedule('quadratic', beta_start, beta_end, num_diffusion_timesteps)\n",
        "\n",
        "class BetaDerivatives():\n",
        "    def __init__(self, betas, dtype=torch.float32):\n",
        "        \"\"\"Take in betas and pre-compute the dependent values to use in forward/ backward pass.\n",
        "\n",
        "        Values are precomputed for all timesteps so that they can be used as and\n",
        "        when required.\n",
        "        \"\"\"\n",
        "        self.np_betas = betas  # Store original numpy array for reference\n",
        "        timesteps, = betas.shape\n",
        "        self.num_timesteps = int(timesteps)\n",
        "\n",
        "        self.betas = torch.tensor(betas, dtype=dtype)\n",
        "        self.alphas = 1. - self.betas\n",
        "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
        "        self.alphas_cumprod_prev = torch.cat([torch.tensor([1.0]), self.alphas_cumprod[:-1]], dim=0)\n",
        "\n",
        "        # Calculations required for diffusion q(x_t | x_{t-1}) and others\n",
        "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
        "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
        "        self.log_one_minus_alphas_cumprod = torch.log(1. - self.alphas_cumprod)\n",
        "\n",
        "    def _gather(self, a, t):\n",
        "        \"\"\"Utility function to extract some coefficients at specified timesteps,\n",
        "        then reshape to [batch_size, 1] for broadcasting.\"\"\"\n",
        "        return a[t].reshape(-1, 1)\n",
        "\n",
        "gdb = BetaDerivatives(betas_linear)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PadYxkN7_QPs"
      },
      "source": [
        "## Forward Pass of Diffusion Model\n",
        "\n",
        "*Given an initial sample* $x_0$ *and a time* $t$, *we can directly compute the noisy sample at time* $t$.\n",
        "\n",
        "In the forward pass, the diffused input at timestep* $t$ *can be computed directly using the closed-form equation:*\n",
        "\n",
        "$$q(x_t| x_0) = N(\\sqrt{\\bar{\\alpha_t}}x_o, 1-\\bar{\\alpha_t}I)$$\n",
        "\n",
        "Here, $\\bar{\\alpha_t}$ represents the cumulative sum of the diffusion alphas up to timestep $t$. This formula is derived from the mathematical framework presented in the paper.\n",
        "\n",
        "The `q_sample` function below implements this equation. It generates a noisy sample at timestep $t$ based on the initial sample $x_0$ and the diffusion process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpJM9cUKQ1XU"
      },
      "outputs": [],
      "source": [
        "class DiffusionForward(BetaDerivatives):\n",
        "    \"\"\"\n",
        "    Forward pass of the diffusion model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, betas):\n",
        "        super().__init__(betas)\n",
        "\n",
        "    def q_sample(self, x_start, t, noise=None):\n",
        "        \"\"\"\n",
        "        Forward pass - sample of diffused data at time t.\n",
        "        \"\"\"\n",
        "        if noise is None:\n",
        "            noise = torch.randn_like(x_start)  # Generate noise with same shape and device as x_start\n",
        "        sqrt_alphas_cumprod = self._gather(self.sqrt_alphas_cumprod, t)\n",
        "        sqrt_one_minus_alphas_cumprod = self._gather(self.sqrt_one_minus_alphas_cumprod, t)\n",
        "        return sqrt_alphas_cumprod * x_start + sqrt_one_minus_alphas_cumprod * noise\n",
        "\n",
        "# Example usage:\n",
        "diff_forward = DiffusionForward(betas_linear)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFxZ2UhF_beI"
      },
      "source": [
        "### Visualize the forward diffusion of the entire data over time\n",
        "\n",
        "We start with original data distribution and move it through the forward diffusion process 10 steps at a time. We can see that the original data distribution information is lost till it resembles gaussian after num_diffusion_steps.\n",
        "\n",
        "Also, the slow perturbations at the start and large ones towards the end as per the beta schedule are evident from the video."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "b7SVU1wNfl15",
        "outputId": "ac2eaef4-efe8-4fe7-8f24-f320f7df7e88"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "camera = Camera(plt.figure())\n",
        "\n",
        "x0 = torch.tensor(train_data)  # Convert to PyTorch tensor\n",
        "for timestep in range(0, num_diffusion_timesteps, 10):\n",
        "    tstep = torch.full((x0.shape[0],), timestep, dtype=torch.long)  # Create timestep tensor\n",
        "    shifted = diff_forward.q_sample(x0, tstep)\n",
        "    plt.scatter(shifted[:, 0].numpy(), shifted[:, 1].numpy(), c=np.arange(x0.shape[0]) // num_samples_per_center)\n",
        "    camera.snap()\n",
        "\n",
        "animation = camera.animate(interval=300)  # Adjust interval as needed\n",
        "animation.save('scatter.mp4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "IwD82p-wvdPw",
        "outputId": "6e887f53-9875-44e0-9237-0eabc6743870"
      },
      "outputs": [],
      "source": [
        "show_video('scatter.mp4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJdCHKvW_BiZ"
      },
      "source": [
        "### Visualize the forward pass of single point\n",
        "\n",
        "We perform the forward diffusion of a single point over time. At every timestep, we generate 500 possible diffused samples of the same input point. We observe the distribution of these points over time.\n",
        "\n",
        "These too are closer to the original point at the start and move towards gaussian as the forward process reaches num_timesteps same as the example above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "eL_o1p_ko2AZ",
        "outputId": "0140f927-36d7-4995-ecfe-94120b85e186"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "camera = Camera(plt.figure())\n",
        "\n",
        "# Define initial positions\n",
        "x0 = torch.tensor([[0.0, 4.0]]).repeat(500, 1)\n",
        "\n",
        "for timestep in range(0, num_diffusion_timesteps, 10):\n",
        "    # Create timestep tensor with repeated values and correct data type\n",
        "    tstep = torch.full((500,), timestep, dtype=torch.long)\n",
        "\n",
        "    # Perform diffusion sampling using the PyTorch model\n",
        "    shifted = diff_forward.q_sample(x0, tstep)\n",
        "\n",
        "    # Plot both shifted and original points with clear labels and legend\n",
        "    plt.scatter(shifted[:, 0], shifted[:, 1], c='b', label=f'Time step: {timestep}')\n",
        "    plt.scatter(x0[:, 0], x0[:, 1], marker='x', c='r', label='Initial')\n",
        "\n",
        "    # Capture frame for the animation\n",
        "    camera.snap()\n",
        "\n",
        "# Save the animation\n",
        "animation = camera.animate(interval=300)  # Adjust interval as needed\n",
        "animation.save('pointshifting.mp4')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "jgu19GXytoQm",
        "outputId": "10521b9c-a4df-4ba3-aff9-c5caf4d52883"
      },
      "outputs": [],
      "source": [
        "show_video('pointshifting.mp4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsSaG3wu28F4"
      },
      "source": [
        "## NN Model\n",
        "\n",
        "With the data preprocessed, we proceed to construct a suitable model for fitting the data. Given the simplicity of our dataset, consisting of only 2 features to reconstruct, we opt for a shallow Deep Neural Network (DNN) with a few layers. However, in the case of image data, we would substitute this architecture with a UNet, which is better suited for handling spatial data, while retaining a similar loss function for optimization.\n",
        "\n",
        "The model takes in 2 inputs:\n",
        "* Timestep embedding of $t$\n",
        "* $x_t$\n",
        "\n",
        "And predicts\n",
        "* The noise $n$ that lead from $x_0$ to $x_t$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iThLbMiT-8Q7"
      },
      "source": [
        "### Timestep Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqW5wDLJSJKr",
        "outputId": "14f3bc98-9df1-478c-df16-b4441f979aa5"
      },
      "outputs": [],
      "source": [
        "# We create a 128 dimensional embedding for the timestep input to the model.\n",
        "# Fixed embeddings similar to positional embeddings in transformer are used -\n",
        "# could be replaced by trainable embeddings later\n",
        "\n",
        "def get_timestep_embedding(timesteps, embedding_dim: int):\n",
        "    half_dim = embedding_dim // 2\n",
        "    emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)\n",
        "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
        "    emb = timesteps.float().unsqueeze(1) * emb.unsqueeze(0)\n",
        "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
        "    if embedding_dim % 2 == 1:  # zero pad\n",
        "        emb = torch.nn.functional.pad(emb, (0, 1), \"constant\", 0)\n",
        "    return emb\n",
        "\n",
        "temb = get_timestep_embedding(torch.tensor([2, 3]), 128)\n",
        "print(temb.shape)\n",
        "print('Embedding for time step 2:\\n', temb[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nDm0rvOQ0VD",
        "outputId": "8542c0ac-d051-4126-9378-eceba9f468ca"
      },
      "outputs": [],
      "source": [
        "# Actual model that takes in x_t and t and outputs n_{t-1}\n",
        "# Experiments showed that prediction of n_{t-1} worked better compared to\n",
        "# prediction of x_{t-1}\n",
        "\n",
        "class NoiseModel(nn.Module):\n",
        "    def __init__(self, input_feat, temb_feat):\n",
        "        super(NoiseModel, self).__init__()\n",
        "        self.d1 = nn.Linear(input_feat, temb_feat)\n",
        "        self.d2 = nn.Sequential(\n",
        "            nn.Linear(temb_feat*2, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_x, temb):\n",
        "        out_d1 = self.d1(input_x)\n",
        "        merged = torch.cat((temb, out_d1), dim=1)\n",
        "        out_d2 = self.d2(merged)\n",
        "        return out_d2\n",
        "\n",
        "# Define input and time embedding dimensions\n",
        "input_feat = train_data.shape[1]\n",
        "temb_feat = 128\n",
        "\n",
        "# Instantiate the model\n",
        "model = NoiseModel(input_feat, temb_feat)\n",
        "\n",
        "print(model) # Equivalent to model.summary() in Keras\n",
        "criterion = nn.MSELoss()  # Equivalent to loss='mse'\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.00005)  # Equivalent to optimizer='adam'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NlpJ_xTmUUFh"
      },
      "source": [
        "### Data Generation for Diffusion Model\n",
        "\n",
        "Next, let's generate the data for the model to train. We generate $x_t$ given the input $x_0$ using the deterministic forward process equation described above. This $x_t$ and timestep embedding of\n",
        "$t$ are input to the model that is tasked with predicting the noise $n$.\n",
        "\n",
        "$t$ is picked uniformly between [0, num_diffusion_timesteps]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5N5v7WL7kB7E"
      },
      "outputs": [],
      "source": [
        "shuffle_buffer_size = 1000\n",
        "batch_size = 32\n",
        "\n",
        "def data_generator_forward(x, gdb):\n",
        "    x=torch.stack(x)\n",
        "    tstep = torch.randint(0, num_diffusion_timesteps, (x.shape[0],), dtype=torch.int32)\n",
        "    noise = torch.randn_like(x)\n",
        "    noisy_out = gdb.q_sample(x, tstep, noise)\n",
        "    timestep_emb = get_timestep_embedding(tstep, 128)\n",
        "    return (noisy_out, timestep_emb), noise\n",
        "\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: data_generator_forward(batch, gdb=diff_forward)  # Pass gdb as an argument\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-lNH2zsUakB",
        "outputId": "28d19811-05b0-447a-911e-c95bc321749e"
      },
      "outputs": [],
      "source": [
        "# Let's test the data generator\n",
        "(xx,tt),yy = next(iter(dataloader))\n",
        "print(xx.shape, tt.shape, yy.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddr-DrYA-x2P"
      },
      "source": [
        "### Train and Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7v01phcpr7M",
        "outputId": "0c57e466-0d69-4a05-fec7-483c380a6ef4"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "    for (input_x, temb), noise in dataloader:\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        predicted_noise = model(input_x, temb)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss = criterion(predicted_noise, noise)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pUhljaJTvpx"
      },
      "source": [
        "### Scatter plots of reconstructed values v/s target\n",
        "\n",
        "When the predictions perfectly match the target values, the scatter plot would form a diagonal line along the line y=x, which represents a perfect alignment between predicted and actual values. In the plot below, we can observe a similar trend, where the points cluster around the line y=x, albeit with some deviation. This behavior indicates that the model's predictions closely align with the target values, suggesting that the model performs reasonably well in predicting the target."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "v8vKw7K4puSL",
        "outputId": "1ed21f54-e6e3-46a3-f48a-e301ced2d92e"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "# Disable gradient calculation for evaluation\n",
        "with torch.no_grad():\n",
        "    # Get a batch of data from the dataloader\n",
        "    ((xx, tt), yy) = next(iter(dataloader))\n",
        "\n",
        "    # Forward pass through the model to get predictions\n",
        "    ypred = model(xx, tt)\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=[12,6])\n",
        "\n",
        "# Plot for dimension 1\n",
        "plt.subplot(1,2,1)\n",
        "plt.scatter(yy[:,0], ypred[:,0].detach())  # Use detach to prevent gradients from being tracked\n",
        "plt.title('Scatter plot prediction (dim1) v/s target')\n",
        "plt.xlabel('Target')\n",
        "plt.ylabel('Prediction')\n",
        "\n",
        "# Plot for dimension 2\n",
        "plt.subplot(1,2,2)\n",
        "plt.scatter(yy[:,1], ypred[:,1].detach())  # Use detach to prevent gradients from being tracked\n",
        "plt.title('Scatter plot prediction (dim2) v/s target')\n",
        "plt.xlabel('Target')\n",
        "plt.ylabel('Prediction')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E85QVROF-i87"
      },
      "source": [
        "## Reverse Diffusion Process (Constructing Image from Noise)\n",
        "\n",
        "The model provides a decent estimate of the noise given the data and t.  Now comes the tricky part: given the data at timestep t $x_t$, and the noise estimate from the model, reconstructing original data distribution.\n",
        "\n",
        "\n",
        "There are 4 parts in the reverse process:\n",
        "1. Pass $x_t$ and $t$ (converted to time embedding) into the model that predicts the noise $œµ$\n",
        "2. Using the noise estimate $œµ$ and $x_t$, compute $x_0$ using equation : $\\frac{1}{\\sqrt{\\bar{\\alpha}_t}}x_t - (\\sqrt{\\frac{1}{\\bar{\\alpha}_t}-1}) \\epsilon$\n",
        "\n",
        "\n",
        "3. Compute mean and variance using the equations:\n",
        "\n",
        "$\\tilde{\\mu}(x_t, x_0) = \\frac{\\sqrt{\\bar{\\alpha}_{t-1}} \\beta_t}{1-\\bar{\\alpha_t}}x_0 + \\frac{\\sqrt{\\bar{\\alpha}_t}(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}x_t$ and\n",
        "variance $\\tilde{\\beta}_t = \\frac{(1-\\bar{\\alpha}_{t-1})}{1-\\bar{\\alpha}_t}\\beta_t$\n",
        "\n",
        "4. Sample using this mean and variance\n",
        "$q(x_{t‚àí1}|x_t, x_0)=N(x_{t‚àí1}; \\tilde{\\mu}(x_t, x_0), \\tilde{\\beta}_tI)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8dw9P9P5q-M"
      },
      "outputs": [],
      "source": [
        "class DiffusionReconstruct(BetaDerivatives):\n",
        "\n",
        "  def __init__(self, betas):\n",
        "    super().__init__(betas)\n",
        "\n",
        "    self.betas = torch.tensor(betas)\n",
        "    self.sqrt_recip_alphas_cumprod = torch.sqrt(1. / self.alphas_cumprod)\n",
        "    self.sqrt_recipm1_alphas_cumprod = torch.sqrt(1. / self.alphas_cumprod - 1)\n",
        "\n",
        "    # Calculations required for posterior q(x_{t-1} | x_t, x_0)\n",
        "    # Variance choice corresponds to 2nd choice mentioned in the paper\n",
        "    self.posterior_variance = self.betas * (1. - self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n",
        "\n",
        "    # Clip the log calculation because the posterior variance is 0 at the beginning of the diffusion chain\n",
        "    self.posterior_log_variance_clipped = torch.log(torch.clamp(self.posterior_variance, min=1e-20))\n",
        "    self.posterior_mean_coef1 = self.betas * torch.sqrt(self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n",
        "    self.posterior_mean_coef2 = (1. - self.alphas_cumprod_prev) * torch.sqrt(self.alphas) / (1. - self.alphas_cumprod)\n",
        "\n",
        "  def predict_start_from_noise(self, x_t, t, noise):\n",
        "        \"\"\"\n",
        "        Reconstruct x_0 using x_t, t and noise. Uses a deterministic process.\n",
        "        \"\"\"\n",
        "        return (\n",
        "            self._gather(self.sqrt_recip_alphas_cumprod, t) * x_t -\n",
        "            self._gather(self.sqrt_recipm1_alphas_cumprod, t) * noise\n",
        "        )\n",
        "\n",
        "  def q_posterior(self, x_start, x_t, t):\n",
        "        \"\"\"\n",
        "        Compute the mean and variance of the diffusion posterior q(x_{t-1} | x_t, x_0)\n",
        "        \"\"\"\n",
        "        posterior_mean = (\n",
        "            self._gather(self.posterior_mean_coef1, t) * x_start +\n",
        "            self._gather(self.posterior_mean_coef2, t) * x_t\n",
        "        )\n",
        "        posterior_log_variance_clipped = self._gather(self.posterior_log_variance_clipped, t)\n",
        "        return posterior_mean, posterior_log_variance_clipped\n",
        "\n",
        "  def p_sample(self, model, x_t, t):\n",
        "        \"\"\"\n",
        "        Sample from the model. This does 4 things:\n",
        "        * Predict the noise from the model using x_t and t\n",
        "        * Create an estimate of x_0 using x_t and noise (reconstruction)\n",
        "        * Estimate the model mean and log_variance of x_{t-1} using x_0, x_t, and t\n",
        "        * Sample data (for x_{t-1}) using the mean and variance values\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "          noise_pred = model(x_t, get_timestep_embedding(t, 128))  # Step 1\n",
        "        x_recon = self.predict_start_from_noise(x_t, t=t, noise=noise_pred)  # Step 2\n",
        "        model_mean, model_log_variance = self.q_posterior(x_start=x_recon, x_t=x_t, t=t)  # Step 3\n",
        "        noise = noise_like(x_t.shape)  # Assuming noise_like is defined elsewhere\n",
        "        nonzero_mask = torch.reshape(torch.gt(t, 0).float(), (x_t.shape[0], 1))\n",
        "        return model_mean + torch.exp(0.5 * model_log_variance) * noise * nonzero_mask  # Step 4\n",
        "\n",
        "  def p_sample_loop_trajectory(self, model, shape):\n",
        "        \"\"\"\n",
        "        Generate the visualization of intermediate steps of the reverse of diffusion process.\n",
        "        \"\"\"\n",
        "        times = torch.tensor([self.num_timesteps - 1], dtype=torch.int32)\n",
        "        imgs = torch.stack([noise_like(shape)])\n",
        "\n",
        "        while times[-1] >= 0:\n",
        "            times = torch.cat([times, torch.tensor([times[-1] - 1])], dim=0)\n",
        "            img = self.p_sample(model=model, x_t=imgs[-1], t=torch.full((shape[0],), times[-1], dtype=torch.int32))\n",
        "            imgs = torch.cat([imgs, img.unsqueeze(0)], dim=0)\n",
        "        return times, imgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhVF29B0ulpq"
      },
      "outputs": [],
      "source": [
        "rec_diff = DiffusionReconstruct(betas_linear)\n",
        "pred_ts, pred_data = rec_diff.p_sample_loop_trajectory(model, shape=(1000,2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvmmtUNoCy9U"
      },
      "source": [
        "### Visualize the Reverse Diffusion Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        },
        "id": "XzZAxflEx-Uu",
        "outputId": "911e5936-f588-46d4-c4aa-554cf3995143"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "camera = Camera(plt.figure(figsize=(10,10)))\n",
        "\n",
        "for timestep in range(0, num_diffusion_timesteps):\n",
        "  plt.scatter(pred_data[timestep][:,0], pred_data[timestep][:,1])\n",
        "  camera.snap()\n",
        "\n",
        "save_animation('outcomes.mp4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Mq3jVa7ekAQ"
      },
      "source": [
        "In the below video, we show the reconstruction process of the data from noise. We start with 100 samples  from std. normal distribution (gaussian noise) and iteratively move towards the original data distribution using the model trained above.\n",
        "\n",
        "As you can see towards the end of the video below, the noise maps back to the original data distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "L5k5G3mLzQHS",
        "outputId": "f01627ad-faac-44e0-a403-ce7b45cf1543"
      },
      "outputs": [],
      "source": [
        "show_video('outcomes.mp4')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxItzOc7FmyF"
      },
      "source": [
        "References:\n",
        "1. [Diffusion models repo](https://github.com/hojonathanho/diffusion)\n",
        "2. [Diffusion models paper](https://arxiv.org/pdf/2006.11239.pdf)\n",
        "3. [Improved Denoising Diffusion Probabilistic Models paper](https://arxiv.org/pdf/2102.09672.pdf)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
